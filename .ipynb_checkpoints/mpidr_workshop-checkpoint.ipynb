{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93e0a1e3-6d46-4005-8157-07a92a5daa32",
   "metadata": {},
   "source": [
    "Title: Working with and Visualizing Geospatial Data\n",
    "\n",
    "Author: Sean Reid\n",
    "\n",
    "Date: 07/07/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae02352-ffd9-4629-a6f8-29ef3e8dd567",
   "metadata": {},
   "source": [
    "# Outline\n",
    "\n",
    "This workshop focuses on working with geospatial data in the following ways:\n",
    "\n",
    "* Types of data and appropriateness for your application - Point, Line, and Polygon\n",
    "* Datums and projections - What to do when things go wrong......\n",
    "* Trusting your data\n",
    "* Modifial areal unit problem\n",
    "* Thinking like a geographer\n",
    "\n",
    "We covered the details of each of these things during the presentation but now we can apply them in practice. We will run through some spatial analysis techniques you can use to investigate the spatial nature of your data. We will cover:\n",
    "* Moran's I\n",
    "* Geographically Weighted Regression (GWR)\n",
    "* Maybe look at an ABM (not a good one) example\n",
    "* Maybe a fun 3D map!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2ed9a7-04c1-47c8-8a90-d5a5301f070f",
   "metadata": {},
   "source": [
    "## Types of Data\n",
    "\n",
    "Understanding the types of spatial data you have is important because each type is unique in how it represents things in the real world. The Open Geospatial Consortium is the gold standard for digital representation of spatial data and most every application uses their standards. [Click here](https://portal.ogc.org/files/?artifact_id=25355) to view the standards for the simple features we are going to talk about today. There are also set standards for spatial processes like distance, buffers, intersections, etc. I like the PostGIS documentation for the data types and spatial processes as well. Use this [link](https://postgis.net/docs/manual-3.2/) if you care to read more about it.\n",
    "\n",
    "Let's start by loading in some example spatial data. We will use some example spatial data from Pysal. Pysal is a great Python library for spatial data analysis. We will use it for both the Moran's I and GWR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d53b2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the necessary modules\n",
    "import os, pandas as pd, geopandas as gpd, numpy as np\n",
    "from libpysal import examples\n",
    "import libpysal as ps\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "435965fd-a816-4b67-b253-6ff652b6b191",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the two datasets we are going to use for this workshop\n",
    "\n",
    "## MSM US county level dataset\n",
    "msm = gpd.read_file(r\"data\\msm_data.shp\")\n",
    "\n",
    "## Quick data cleaning to ensure things run smoothly\n",
    "msm['same_sex_m'] = msm['same_sex_m'].astype('float')\n",
    "msm['total_hh'] = msm['total_hh'].astype('float')\n",
    "msm = msm.rename(columns={\"index\":\"msm_index\"})\n",
    "\n",
    "## Berlin Dataset\n",
    "ber = gpd.read_file(ps.examples.get_path(\"prenzlauer.zip\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e494fe-a50f-48c4-be7d-372ed8899887",
   "metadata": {},
   "source": [
    "The MSM US dataset consists of all US counties derived from 2018 ACS 5 year data. It has information on number of same sex male households by county, voting history, population, urbanicity, income, and LGBTQ community index.\n",
    "\n",
    "The Berlin dataset consists of Airbnb rental properties in the Prenzlauer Berg area of the city. The dataset contains information on how many people it can host, review scores, bedrooms, bathrooms, and price per night. This dataset also has X and Y coordinates which will come in handy later.\n",
    "\n",
    "Let's plot the data using geopandas mapping functionality to see what our data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a214c8-98b0-4279-9220-ed1713a03c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map the MSM data\n",
    "msm.plot(figsize=(8,8), alpha=0.75, edgecolor='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b89fc51-1e16-4e00-837d-83379f3849b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Map the Berlin Data\n",
    "ber.plot(figsize=(8,8), alpha=0.75, edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490679b6-2c26-41d6-9d56-c1163b26f57a",
   "metadata": {},
   "source": [
    "Awesome, we can see that our MSM data is a polygon dataset and our Berlin dataset is a point dataset. This will influence how we think about the data moving forward and what we can do with it. These plots are a nice start but they don't give much context about what is around it. So let's add a basemap to give some context!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc508fb-86bb-44cb-a834-f112d17f5b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add a basemap to the maps above\n",
    "## Import the module contextily which is commonly used in these situations\n",
    "import contextily as cx\n",
    "\n",
    "## Plot the Berlin data but with a basemap this time\n",
    "ax = ber.plot(figsize=(8,8), alpha=0.75, edgecolor='black')\n",
    "cx.add_basemap(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9661a0-2005-457b-a6d8-a4ce44eb83a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the MSM data but with a basemap this time\n",
    "ax = msm.plot(figsize=(8,8), alpha=0.75, edgecolor='black')\n",
    "cx.add_basemap(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c151d4-1ff0-4e7e-b3fe-42b06962efe9",
   "metadata": {},
   "source": [
    "### Projection Issues\n",
    "\n",
    "The Berlin data looks great on the basemap but what in the world is going on with the MSM US data????? Let's check the Coordinate reference systems for both to see what's going on......."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be06199-fcd2-456d-8fb8-95fd771af88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check CRS for each Georgia and Berlin\n",
    "print(msm.crs)\n",
    "print(ber.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f694c67c-c80c-42b0-a812-ffd9642a3390",
   "metadata": {},
   "source": [
    "Ahhhhhh okay this makes more sense now. The MSM data is in the wrong reference system, so it just got plopped in the wrong place on the basemap. The Berlin data is in a Web Mercator projection, which is consistent with the basemap. You might think to yourself \"let's use a WGS 84 projection\" but that uses different units than Web Mercator. So to ensure the msm data lines up with the basemap, we will assign it to EPSG 3857."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a6463f-7722-49d3-99e2-a237303d73e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "msm = msm.to_crs(epsg=3857)\n",
    "\n",
    "# Plot the Georgia data but with a basemap this time\n",
    "ax1 = msm.plot(figsize=(8,8), alpha=0.75, edgecolor='black')\n",
    "cx.add_basemap(ax1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51f8399-33f3-4d27-8d80-45e983f9794f",
   "metadata": {},
   "source": [
    "Okay, that looks better and we have things lined up. Sometimes this doesn't work so nicely when the data doesn't have a CRS or projection to start with. I tried setting this tutorial up using a different Pysal dataset and I couldn't figure out how to correct the coordinate system. It can be frustrating when the spatial information for a dataset is whack...... Data quality is important. It could also have been operator error on not being able to fix the projection but who knows.......\n",
    "\n",
    "## Moran's I\n",
    "Now we are going going to look at the spatial structure of our data by running global and local Moran's I calculations. These tests will tell us if there is significant clustering in our dependent variables. In the case of our two examples they are price per night and same sex male households for the Berlin Airbnb and MSM data respectively.\n",
    "\n",
    "First we need to generate a spatial weights matrix which will be done using Pysal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fdd0c0-0be4-4d0b-a627-f7016faba765",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the modules necessary for this task\n",
    "from libpysal.weights.contiguity import Queen\n",
    "from esda.moran import Moran\n",
    "from splot.esda import moran_scatterplot\n",
    "from splot.esda import plot_moran\n",
    "from splot.esda import lisa_cluster\n",
    "\n",
    "## Set the spatial weights matrix for the msm dataset\n",
    "y_msm = msm['same_sex_m']\n",
    "w_msm = Queen.from_dataframe(msm)\n",
    "\n",
    "## This is row standardizing the weights matrix so everything sums to 1\n",
    "w_msm.transform = 'r'\n",
    "\n",
    "## Set the spatial weights for the Berlin dataset\n",
    "y_ber = ber['price']\n",
    "w_ber = Queen.from_dataframe(ber)\n",
    "\n",
    "## This is row standardizing the weights matrix so everything sums to 1\n",
    "w_ber.transform = 'r'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ebf6f6-1492-4cad-979e-bee0ddf1bb06",
   "metadata": {},
   "source": [
    "As you can see above, we have some islands which will cause issues for some local clustering we want to do later. We can simply remove the islands from our dataset or we can convert our polygon data into point data and create a spatial weights matrix that way. It's a sly solution but can introduce it's own unique problems. Let's take a look at this a bit later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a584c5a9-924e-4b7b-afcc-b20898b2311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now we will calculate the Global Moran's I for each dataset\n",
    "\n",
    "## Generate Global Moran's I for MSM dataset\n",
    "\n",
    "## Calculate Moran's I\n",
    "moran_msm = Moran(y_msm,w_msm)\n",
    "\n",
    "## Print moran's I value and p value\n",
    "print(\"Moran's I value is:\",moran_msm.I)\n",
    "print(\"P value is:\",moran_msm.p_sim)\n",
    "plot_moran(moran_msm, zstandard=True, figsize=(10,3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d772ec9f-8d5e-45f9-8737-c8a5d94cb538",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate Global Moran's I for Berlin Airbnb dataset\n",
    "\n",
    "## Calculate Moran's I\n",
    "moran_ber = Moran(y_ber,w_ber)\n",
    "\n",
    "## Print moran's I value and p value\n",
    "print(\"Moran's I value is:\",moran_ber.I)\n",
    "print(\"P value is:\",moran_ber.p_sim)\n",
    "plot_moran(moran_ber, zstandard=True, figsize=(10,3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7317df-70eb-4e92-b48c-e25310998e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We some evidence of spatial autocorrelation in our data so we want to take a look at our local Moran's I and create LISA (Local Inidicator of Spatial Autocorrelation).\n",
    "\n",
    "\n",
    "## Import necessary modules\n",
    "from esda.moran import Moran_Local\n",
    "\n",
    "## Generate Local Moran's I and LISA Clusters for each dataset\n",
    "\n",
    "## Local Moran's I for MSM dataset\n",
    "moran_loc_msm = Moran_Local(y_msm,w_msm)\n",
    "fig, ax = moran_scatterplot(moran_loc_msm, p=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41eb85fe-c6f1-4789-af3c-17fe2bab4a11",
   "metadata": {},
   "source": [
    "The error above occured because we have islands in our data and that is causing the code to stall. Removing islands is a perfectly reasonable thing to do here but I want to take a moment to show you why data type is important and how it can help you solve some problems if you know how to visually inspect your data.\n",
    "\n",
    "First I am going to show you the connections between counties generated by the polygon derived weights matrix. Then we are going to generate another weights matrix from centroids of polygons and use that to calculate the local moran's I for counties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4db4d1-d2cd-40e3-80f4-5f6fd382192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's take a look at the spatial representation of our weights matrix\n",
    "\n",
    "## Create a centroid in our dataset for visualization purposes (and we will need it later)\n",
    "msm['centroid'] = msm.centroid\n",
    "\n",
    "## Generate an numpy array of the centroids\n",
    "centroids = np.array([list((i.x,i.y)) for i in msm['centroid']])\n",
    "\n",
    "## Split the x and y coordinates into separate lists\n",
    "xpoints = []\n",
    "ypoints = []\n",
    "for i in msm['centroid']:\n",
    "    xpoints.append(i.x)\n",
    "    ypoints.append(i.y)\n",
    "    \n",
    "## Make the connections between points\n",
    "plt.plot(centroids[:,0], centroids[:,1],'.')\n",
    "for k,neighs in w_msm.neighbors.items():\n",
    "    origin = centroids[k]\n",
    "    for neigh in neighs:\n",
    "        segment = centroids[[k,neigh]]\n",
    "        plt.plot(segment[:,0], segment[:,1], '-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e100a-8a89-43a6-9efa-c17e28683978",
   "metadata": {},
   "source": [
    "Above we can see the islands that are causing us issues but the connections between counties look about right! It's a bit of a mess but it looks okay.\n",
    "\n",
    "Let's see what this would look like if we used points instead of polygons to create our spatial weights matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd0a92c-4f8c-4973-8a2a-2d752b3c69dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the geometry from the polygons to the points\n",
    "msm = msm.set_geometry('centroid')\n",
    "\n",
    "## Recreate the weights\n",
    "\n",
    "## Set the spatial weights matrix for the msm dataset\n",
    "y_msm = msm['same_sex_m']\n",
    "w_msm = Queen.from_dataframe(msm)\n",
    "\n",
    "## This is row standardizing the weights matrix so everything sums to 1\n",
    "w_msm.transform = 'r'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2741a7cd-602e-40b8-a4a2-b5fb8bdeab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now let's see how things look by mapping the connections again\n",
    "\n",
    "## Generate an numpy array of the centroids\n",
    "centroids = np.array([list((i.x,i.y)) for i in msm['centroid']])\n",
    "\n",
    "## Split the x and y coordinates into separate lists\n",
    "xpoints = []\n",
    "ypoints = []\n",
    "for i in msm['centroid']:\n",
    "    xpoints.append(i.x)\n",
    "    ypoints.append(i.y)\n",
    "    \n",
    "## Make the connections between points\n",
    "plt.plot(centroids[:,0], centroids[:,1],'.')\n",
    "for k,neighs in w_msm.neighbors.items():\n",
    "    origin = centroids[k]\n",
    "    for neigh in neighs:\n",
    "        segment = centroids[[k,neigh]]\n",
    "        plt.plot(segment[:,0], segment[:,1], '-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc780fd9-1a7c-4cf9-a70a-e8fcea636140",
   "metadata": {},
   "source": [
    "Looks similar but now we have some really weird connections between places that should not be connected. Hawaii and Alaska are connected to the contiguous US and probably shouldn't be. You can tinker with the parameters here and make a judgement call based on your unique situation but it's important to consider the type of spatial data you are using and how it's influencing your results.\n",
    "\n",
    "I'm not super stoked on this matrix but it won't break my code so let's use it for now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc9bec6-58ea-4416-bcf9-d5fb7665d9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Local Moran's I for MSM dataset\n",
    "moran_loc_msm = Moran_Local(y_msm,w_msm)\n",
    "fig, ax = moran_scatterplot(moran_loc_msm, p=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e9203b-12f6-45d0-a010-63a27e7edee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Local Moran's I for Berlin Airbnb dataset\n",
    "moran_loc_ber = Moran_Local(y_ber,w_ber)\n",
    "fig, ax = moran_scatterplot(moran_loc_ber, p=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad84b11-69a4-4300-99ed-2a3bb68b569f",
   "metadata": {},
   "source": [
    "Okay, now we have a local moran scatter plot for each data set with the values that are significant highlighted in 4 colors. \n",
    "* Red = High values clustered near other high values\n",
    "* Orange = High values clustered near low values\n",
    "* Light Blue = Low values clustered near high values\n",
    "* Dark Blue = Low values clustered near low values\n",
    "\n",
    "We can also map this using a LISA cluster rather than looking at it on a plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b264f6e-5104-4852-97fd-f2f36e8f5b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a LISA cluster for each dataset\n",
    "\n",
    "## LISA for MSM data\n",
    "## Set geometry back to polygons\n",
    "msm = msm.set_geometry('geometry')\n",
    "msm = msm.to_crs(epsg=2163)\n",
    "fig,ax = lisa_cluster(moran_loc_msm,msm,p=0.05,figsize=(12,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1ea17a-e288-4778-a748-180ddef31b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LISA for Berlin Airbnb Data\n",
    "fix,ax = lisa_cluster(moran_loc_ber,ber,p=0.05,figsize=(12,12))\n",
    "cx.add_basemap(ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bdd59c-abcc-4a59-b0e3-1b7ea1bcf51a",
   "metadata": {},
   "source": [
    "### Geographically Weighted Regression (GWR)\n",
    "\n",
    "Finally, we are going to run a geographically weighted regression model on our two datasets. This model assumes that there are non-stationary relationships in the data where the relationship between the independent and dependent variables changes based on location.\n",
    "\n",
    "This type of analysis allows researchers to test Tobler's first law of geography in real world settings. Like we talked about during the presentation; space can be thought of in ever changing ways as technology and innovation continue to push the boundaries of geographic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d72494-ec30-4448-9b1b-0e9253190d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary modules for GWR\n",
    "from pysal.model import mgwr\n",
    "import patsy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978d229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate our standardized dataset for GWR Model\n",
    "\n",
    "## Equation for MSM dataset\n",
    "y_msm,X_msm = patsy.dmatrices('''standardize(same_sex_m) ~ standardize(over_18_me) + standardize(total_hh) + standardize(median_inc) + standardize(pvi) + standardize(msm_index)''', data=msm)\n",
    "\n",
    "## Equation for Berlin Dataset\n",
    "y_ber,X_ber = patsy.dmatrices('''standardize(price) ~ standardize(accommodat) + standardize(review_sco) + standardize(bedrooms) + standardize(bathrooms) + standardize(beds)''', data=ber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402e479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set coordinates for the next section\n",
    "\n",
    "## MSM Data\n",
    "coords_msm = np.column_stack([msm['centroid'].to_crs(epsg=3857).x,msm['centroid'].to_crs(epsg=3857).y])\n",
    "\n",
    "## Berlin Airbnb Data\n",
    "ber['centroid'] = ber.centroid\n",
    "coords_ber = np.column_stack([ber['centroid'].to_crs(epsg=3857).x,ber['centroid'].to_crs(epsg=3857).y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245facdb-907b-45ea-b43b-c3626af955bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the bandwidth parameter\n",
    "import warnings\n",
    "\n",
    "## Set bandwidth for MSM data\n",
    "warnings.filterwarnings('ignore')\n",
    "bw_msm = mgwr.sel_bw.Sel_BW(coords_msm,np.asarray(y_msm),np.asarray(X_msm),fixed=False,spherical=True)\n",
    "bw_msm.search()\n",
    "\n",
    "## Set bandwidth for Berlin Airbnb Data\n",
    "bw_ber = mgwr.sel_bw.Sel_BW(coords_ber,np.asarray(y_ber),np.asarray(X_ber),fixed=False,spherical=True)\n",
    "bw_ber.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e5f187-38a9-49fa-9a9b-f156aef23999",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize and fit the model for each dataset\n",
    "\n",
    "## MSM Data\n",
    "gwr_model_msm = mgwr.gwr.GWR(coords_msm,np.asarray(y_msm),np.asarray(X_msm),bw_msm.bw[0])\n",
    "\n",
    "results_msm = gwr_model_msm.fit()\n",
    "\n",
    "results_msm.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5b58f9-0c7e-4ca1-81fb-d6b215ba6683",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Berlin Airbnb Data\n",
    "gwr_model_ber = mgwr.gwr.GWR(coords_ber,np.asarray(y_ber),np.asarray(X_ber),bw_ber.bw[0])\n",
    "\n",
    "results_ber = gwr_model_ber.fit()\n",
    "\n",
    "results_ber.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985b4adc-5bea-4846-a90a-dff77eca3cab",
   "metadata": {},
   "source": [
    "These outputs are interesting but can be quite difficult to interpret. We are generally interested in the spread coefficient estimates. They can be plotted or put into a table but since we are interested in spatial relationships, we are going to map it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba5e462-02b6-49a4-9be3-da72593b8aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter our estimates for significance\n",
    "\n",
    "## MSM Data\n",
    "filtered_estimates_msm = results_msm.filter_tvals(alpha=.05)\n",
    "\n",
    "## Berlin Airbnb Data\n",
    "filtered_estimates_ber = results_ber.filter_tvals(alpha=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51b0b61-7f66-4b37-80b9-36ff2d1fa1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's plot these significance values in a map\n",
    "\n",
    "## MSM Data\n",
    "f,ax = plt.subplots(6,1,figsize=(20,30),subplot_kw=dict(aspect='equal'))\n",
    "ax = ax.flatten()\n",
    "\n",
    "## first we are going to work with each of the covariates\n",
    "for i,row in enumerate(filtered_estimates_msm.T):\n",
    "    ## make a new column in the data dataframe\n",
    "    data_tmp = msm.assign(toplot = filtered_estimates_msm.T[i])\n",
    "    \n",
    "#     data_tmp = data_tmp[~data_tmp['state_x'].isin(['02','15'])]\n",
    "    \n",
    "    ## now we want to plot all non significant values in light gray\n",
    "    data_tmp.query('toplot == 0').sort_values('toplot').plot(color='grey', marker='.', ax=ax[i])\n",
    "    \n",
    "    ## plot all the significant estimates in color\n",
    "    data_tmp.query('toplot != 0').sort_values('toplot').plot('toplot',cmap='plasma',marker='.', ax=ax[i],legend=True)\n",
    "    \n",
    "\n",
    "    ax[i].set_title(['Constant','Over 18 Men','Total HH','Median Income','PVI',\n",
    "                    'LGBTQ Community Index'][i],fontsize=20)\n",
    "    \n",
    "    ## Remove lat and lon ticks and labels\n",
    "    ax[i].set_xticklabels([])\n",
    "    ax[i].set_yticklabels([])\n",
    "    ax[i].set_xticks([])\n",
    "    ax[i].set_yticks([])\n",
    "    \n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d3235c-5dc8-445b-98ca-8891932c4dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Berlin Airbnb Data\n",
    "f,ax = plt.subplots(6,1,figsize=(20,30),subplot_kw=dict(aspect='equal'))\n",
    "ax = ax.flatten()\n",
    "\n",
    "## first we are going to work with each of the covariates\n",
    "for i,row in enumerate(filtered_estimates_ber.T):\n",
    "    ## make a new column in the data dataframe\n",
    "    data_tmp = ber.assign(toplot = filtered_estimates_ber.T[i])\n",
    "    \n",
    "    \n",
    "    ## now we want to plot all non significant values in light gray\n",
    "    data_tmp.query('toplot == 0').sort_values('toplot').plot(color='grey', marker='.', ax=ax[i])\n",
    "    \n",
    "    ## plot all the significant estimates in color\n",
    "    data_tmp.query('toplot != 0').sort_values('toplot').plot('toplot',cmap='plasma',marker='.', ax=ax[i],legend=True)\n",
    "    \n",
    "    \n",
    "    ax[i].set_title(['Constant','Accommodation','Review Score','Bedrooms','Bathrooms',\n",
    "                    'Beds'][i],fontsize=20)\n",
    "    \n",
    "    ## Remove lat and lon ticks and labels\n",
    "    ax[i].set_xticklabels([])\n",
    "    ax[i].set_yticklabels([])\n",
    "    ax[i].set_xticks([])\n",
    "    ax[i].set_yticks([])\n",
    "    cx.add_basemap(ax[i])\n",
    "    \n",
    "f.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6bcfe2-d008-4386-931f-32f963d41c12",
   "metadata": {},
   "source": [
    "In both datasets we see spatially varying relationships where some covariates are positively associated with the dependent variables and are negatively associated in others. These examples are simplistic and need much more formal framing/background knowledge for this type of analysis to be useful. Going through the exercise and seeing the possibilities are worth seeing I think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec10260-6f8e-44d7-b3b4-1e3bca91b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finally we can plot the R2 for each point/polygon in the regression model\n",
    "\n",
    "## MSM Data\n",
    "fig, ax = plt.subplots(1,figsize=(20,8))\n",
    "ax = msm.assign(r2=results_msm.localR2).sort_values('r2').plot('r2',ax=ax,legend=True,marker='.',vmin=0,vmax=1,cmap='Reds')\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159a8132-16ff-440c-8a0b-bc4812ed8dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Berlin Airbnb Data\n",
    "fig, ax = plt.subplots(1,figsize=(20,8))\n",
    "ax = ber.assign(r2=results_ber.localR2).sort_values('r2').plot('r2',ax=ax,legend=True,marker='.',vmin=0,vmax=1,cmap='Reds')\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "cx.add_basemap(ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7957d224-1e07-4e41-9ee1-233a102dc4b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
